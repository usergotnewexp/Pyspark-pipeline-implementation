{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw0mvT0UM0m3TrzrAW3CTo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usergotnewexp/Pyspark-pipeline-implementation/blob/main/Pyspark_pipeline_imp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUfkY4dB4Nit",
        "outputId": "6a9a9c21-3e3a-4969-f79b-ba0044daa587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 1.0.1 requires pyspark[connect]~=4.0.0, but you have pyspark 3.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSpark version: 3.5.1\n",
            "+---+------+-------+-----------+-------+--------+\n",
            "| id|  name|country|signup_date| income|    plan|\n",
            "+---+------+-------+-----------+-------+--------+\n",
            "|  1| Alice|     IN| 2025-10-01|56000.0| premium|\n",
            "|  2|   Bob|     US| 2025-10-03|43000.0|standard|\n",
            "|  3|Carlos|     IN| 2025-09-27|72000.0| premium|\n",
            "|  4| Diana|     UK| 2025-09-30|39000.0|standard|\n",
            "|  5|  Esha|     IN| 2025-10-02|85000.0| premium|\n",
            "|  6| Farid|     AE| 2025-10-02|31000.0|   basic|\n",
            "|  7|  Gita|     IN| 2025-09-29|46000.0|standard|\n",
            "|  8|Hassan|     PK| 2025-10-01|52000.0| premium|\n",
            "+---+------+-------+-----------+-------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pyspark==3.5.1\n",
        "from pyspark.sql import SparkSession, functions as F, Window\n",
        "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, FloatType\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "spark = (SparkSession.builder.appName(\"ColabSparkAdvancedTutorial\")\n",
        "         .master(\"local[*]\")\n",
        "         .config(\"spark.sql.shuffle.partitions\", \"4\")\n",
        "         .getOrCreate())\n",
        "print(\"Spark version:\", spark.version)\n",
        "\n",
        "data = [\n",
        "    (1, \"Alice\", \"IN\", \"2025-10-01\", 56000.0, \"premium\"),\n",
        "    (2, \"Bob\", \"US\", \"2025-10-03\", 43000.0, \"standard\"),\n",
        "    (3, \"Carlos\", \"IN\", \"2025-09-27\", 72000.0, \"premium\"),\n",
        "    (4, \"Diana\", \"UK\", \"2025-09-30\", 39000.0, \"standard\"),\n",
        "    (5, \"Esha\", \"IN\", \"2025-10-02\", 85000.0, \"premium\"),\n",
        "    (6, \"Farid\", \"AE\", \"2025-10-02\", 31000.0, \"basic\"),\n",
        "    (7, \"Gita\", \"IN\", \"2025-09-29\", 46000.0, \"standard\"),\n",
        "    (8, \"Hassan\", \"PK\", \"2025-10-01\", 52000.0, \"premium\"),\n",
        "]\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), False),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"signup_date\", StringType(), True),\n",
        "    StructField(\"income\", FloatType(), True),\n",
        "    StructField(\"plan\", StringType(), True),\n",
        "])\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = (df.withColumn(\"signup_ts\", F.to_timestamp(\"signup_date\"))\n",
        "         .withColumn(\"year\", F.year(\"signup_ts\"))\n",
        "         .withColumn(\"month\", F.month(\"signup_ts\"))\n",
        "         .withColumn(\"is_india\", (F.col(\"country\") == \"IN\").cast(\"int\")))\n",
        "df2.show()\n",
        "\n",
        "df2.createOrReplaceTempView(\"users\")\n",
        "spark.sql(\"\"\"\n",
        "SELECT country, COUNT(*) AS cnt, AVG(income) AS avg_income\n",
        "FROM users\n",
        "GROUP BY country\n",
        "ORDER BY cnt DESC\n",
        "\"\"\").show()\n",
        "\n",
        "w = Window.partitionBy(\"country\").orderBy(F.col(\"income\").desc())\n",
        "df_ranked = df2.withColumn(\"income_rank_in_country\", F.rank().over(w))\n",
        "df_ranked.show()\n",
        "\n",
        "def plan_priority(plan):\n",
        "    if plan == \"premium\": return 3\n",
        "    if plan == \"standard\": return 2\n",
        "    if plan == \"basic\": return 1\n",
        "    return 0\n",
        "plan_priority_udf = F.udf(plan_priority, IntegerType())\n",
        "df_udf = df_ranked.withColumn(\"plan_priority\", plan_priority_udf(F.col(\"plan\")))\n",
        "df_udf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdYDmdJg4O7L",
        "outputId": "5713d8b2-7d22-462c-fe4c-2b36ab1c2888"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+-------+-----------+-------+--------+-------------------+----+-----+--------+\n",
            "| id|  name|country|signup_date| income|    plan|          signup_ts|year|month|is_india|\n",
            "+---+------+-------+-----------+-------+--------+-------------------+----+-----+--------+\n",
            "|  1| Alice|     IN| 2025-10-01|56000.0| premium|2025-10-01 00:00:00|2025|   10|       1|\n",
            "|  2|   Bob|     US| 2025-10-03|43000.0|standard|2025-10-03 00:00:00|2025|   10|       0|\n",
            "|  3|Carlos|     IN| 2025-09-27|72000.0| premium|2025-09-27 00:00:00|2025|    9|       1|\n",
            "|  4| Diana|     UK| 2025-09-30|39000.0|standard|2025-09-30 00:00:00|2025|    9|       0|\n",
            "|  5|  Esha|     IN| 2025-10-02|85000.0| premium|2025-10-02 00:00:00|2025|   10|       1|\n",
            "|  6| Farid|     AE| 2025-10-02|31000.0|   basic|2025-10-02 00:00:00|2025|   10|       0|\n",
            "|  7|  Gita|     IN| 2025-09-29|46000.0|standard|2025-09-29 00:00:00|2025|    9|       1|\n",
            "|  8|Hassan|     PK| 2025-10-01|52000.0| premium|2025-10-01 00:00:00|2025|   10|       0|\n",
            "+---+------+-------+-----------+-------+--------+-------------------+----+-----+--------+\n",
            "\n",
            "+-------+---+----------+\n",
            "|country|cnt|avg_income|\n",
            "+-------+---+----------+\n",
            "|     IN|  4|   64750.0|\n",
            "|     US|  1|   43000.0|\n",
            "|     UK|  1|   39000.0|\n",
            "|     PK|  1|   52000.0|\n",
            "|     AE|  1|   31000.0|\n",
            "+-------+---+----------+\n",
            "\n",
            "+---+------+-------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+\n",
            "| id|  name|country|signup_date| income|    plan|          signup_ts|year|month|is_india|income_rank_in_country|\n",
            "+---+------+-------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+\n",
            "|  6| Farid|     AE| 2025-10-02|31000.0|   basic|2025-10-02 00:00:00|2025|   10|       0|                     1|\n",
            "|  5|  Esha|     IN| 2025-10-02|85000.0| premium|2025-10-02 00:00:00|2025|   10|       1|                     1|\n",
            "|  3|Carlos|     IN| 2025-09-27|72000.0| premium|2025-09-27 00:00:00|2025|    9|       1|                     2|\n",
            "|  1| Alice|     IN| 2025-10-01|56000.0| premium|2025-10-01 00:00:00|2025|   10|       1|                     3|\n",
            "|  7|  Gita|     IN| 2025-09-29|46000.0|standard|2025-09-29 00:00:00|2025|    9|       1|                     4|\n",
            "|  8|Hassan|     PK| 2025-10-01|52000.0| premium|2025-10-01 00:00:00|2025|   10|       0|                     1|\n",
            "|  4| Diana|     UK| 2025-09-30|39000.0|standard|2025-09-30 00:00:00|2025|    9|       0|                     1|\n",
            "|  2|   Bob|     US| 2025-10-03|43000.0|standard|2025-10-03 00:00:00|2025|   10|       0|                     1|\n",
            "+---+------+-------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+\n",
            "\n",
            "+---+------+-------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+-------------+\n",
            "| id|  name|country|signup_date| income|    plan|          signup_ts|year|month|is_india|income_rank_in_country|plan_priority|\n",
            "+---+------+-------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+-------------+\n",
            "|  6| Farid|     AE| 2025-10-02|31000.0|   basic|2025-10-02 00:00:00|2025|   10|       0|                     1|            1|\n",
            "|  5|  Esha|     IN| 2025-10-02|85000.0| premium|2025-10-02 00:00:00|2025|   10|       1|                     1|            3|\n",
            "|  3|Carlos|     IN| 2025-09-27|72000.0| premium|2025-09-27 00:00:00|2025|    9|       1|                     2|            3|\n",
            "|  1| Alice|     IN| 2025-10-01|56000.0| premium|2025-10-01 00:00:00|2025|   10|       1|                     3|            3|\n",
            "|  7|  Gita|     IN| 2025-09-29|46000.0|standard|2025-09-29 00:00:00|2025|    9|       1|                     4|            2|\n",
            "|  8|Hassan|     PK| 2025-10-01|52000.0| premium|2025-10-01 00:00:00|2025|   10|       0|                     1|            3|\n",
            "|  4| Diana|     UK| 2025-09-30|39000.0|standard|2025-09-30 00:00:00|2025|    9|       0|                     1|            2|\n",
            "|  2|   Bob|     US| 2025-10-03|43000.0|standard|2025-10-03 00:00:00|2025|   10|       0|                     1|            2|\n",
            "+---+------+-------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "country_data = [\n",
        "    (\"IN\", \"Asia\", 1.42), (\"US\", \"North America\", 0.33),\n",
        "    (\"UK\", \"Europe\", 0.07), (\"AE\", \"Asia\", 0.01), (\"PK\", \"Asia\", 0.24),\n",
        "]\n",
        "country_schema = StructType([\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"region\", StringType(), True),\n",
        "    StructField(\"population_bn\", FloatType(), True),\n",
        "])\n",
        "country_df = spark.createDataFrame(country_data, country_schema)\n",
        "\n",
        "joined = df_udf.alias(\"u\").join(country_df.alias(\"c\"), on=\"country\", how=\"left\")\n",
        "joined.show()\n",
        "\n",
        "region_stats = (joined.groupBy(\"region\", \"plan\")\n",
        "                .agg(F.count(\"*\").alias(\"users\"),\n",
        "                     F.round(F.avg(\"income\"), 2).alias(\"avg_income\"))\n",
        "                .orderBy(\"region\", \"plan\"))\n",
        "region_stats.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNKGWxQn4jPf",
        "outputId": "b7f36415-5923-4339-941b-2fb76b7662f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+-------------+-------------+-------------+\n",
            "|country| id|  name|signup_date| income|    plan|          signup_ts|year|month|is_india|income_rank_in_country|plan_priority|       region|population_bn|\n",
            "+-------+---+------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+-------------+-------------+-------------+\n",
            "|     PK|  8|Hassan| 2025-10-01|52000.0| premium|2025-10-01 00:00:00|2025|   10|       0|                     1|            3|         Asia|         0.24|\n",
            "|     UK|  4| Diana| 2025-09-30|39000.0|standard|2025-09-30 00:00:00|2025|    9|       0|                     1|            2|       Europe|         0.07|\n",
            "|     US|  2|   Bob| 2025-10-03|43000.0|standard|2025-10-03 00:00:00|2025|   10|       0|                     1|            2|North America|         0.33|\n",
            "|     AE|  6| Farid| 2025-10-02|31000.0|   basic|2025-10-02 00:00:00|2025|   10|       0|                     1|            1|         Asia|         0.01|\n",
            "|     IN|  5|  Esha| 2025-10-02|85000.0| premium|2025-10-02 00:00:00|2025|   10|       1|                     1|            3|         Asia|         1.42|\n",
            "|     IN|  3|Carlos| 2025-09-27|72000.0| premium|2025-09-27 00:00:00|2025|    9|       1|                     2|            3|         Asia|         1.42|\n",
            "|     IN|  1| Alice| 2025-10-01|56000.0| premium|2025-10-01 00:00:00|2025|   10|       1|                     3|            3|         Asia|         1.42|\n",
            "|     IN|  7|  Gita| 2025-09-29|46000.0|standard|2025-09-29 00:00:00|2025|    9|       1|                     4|            2|         Asia|         1.42|\n",
            "+-------+---+------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+-------------+-------------+-------------+\n",
            "\n",
            "+-------------+--------+-----+----------+\n",
            "|       region|    plan|users|avg_income|\n",
            "+-------------+--------+-----+----------+\n",
            "|         Asia|   basic|    1|   31000.0|\n",
            "|         Asia| premium|    4|   66250.0|\n",
            "|         Asia|standard|    1|   46000.0|\n",
            "|       Europe|standard|    1|   39000.0|\n",
            "|North America|standard|    1|   43000.0|\n",
            "+-------------+--------+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ml_df = joined.withColumn(\"label\", (F.col(\"plan\") == \"premium\").cast(\"int\")).na.drop()\n",
        "country_indexer = StringIndexer(inputCol=\"country\", outputCol=\"country_idx\", handleInvalid=\"keep\")\n",
        "country_fitted = country_indexer.fit(ml_df)\n",
        "ml_df2 = country_fitted.transform(ml_df)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"income\", \"country_idx\", \"plan_priority\"], outputCol=\"features\")\n",
        "ml_final = assembler.transform(ml_df2)\n",
        "train_df, test_df = ml_final.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20)\n",
        "lr_model = lr.fit(train_df)\n",
        "preds = lr_model.transform(test_df)\n",
        "preds.select(\"name\", \"country\", \"income\", \"plan\", \"label\", \"prediction\", \"probability\").show(truncate=False)\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "acc = evaluator.evaluate(preds)\n",
        "print(\"Classification accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJQ86qLs4tW5",
        "outputId": "c6c5e670-b01d-494b-bdb0-288e58cf9e77"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-------+--------+-----+----------+-----------------------------------------+\n",
            "|name  |country|income |plan    |label|prediction|probability                              |\n",
            "+------+-------+-------+--------+-----+----------+-----------------------------------------+\n",
            "|Carlos|IN     |72000.0|premium |1    |1.0       |[5.631679339875724E-9,0.9999999943683207]|\n",
            "|Diana |UK     |39000.0|standard|0    |0.0       |[0.9999997637769654,2.362230345775984E-7]|\n",
            "+------+-------+-------+--------+-----+----------+-----------------------------------------+\n",
            "\n",
            "Classification accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_path = \"/content/spark_users_parquet\"\n",
        "joined.write.mode(\"overwrite\").parquet(output_path)\n",
        "parquet_df = spark.read.parquet(output_path)\n",
        "print(\"Parquet reloaded:\")\n",
        "parquet_df.show()\n",
        "\n",
        "recent = spark.sql(\"\"\"\n",
        "SELECT name, country, income, signup_ts\n",
        "FROM users\n",
        "WHERE signup_ts >= '2025-10-01'\n",
        "ORDER BY signup_ts DESC\n",
        "\"\"\")\n",
        "recent.show()\n",
        "\n",
        "recent.explain()\n",
        "spark.stop()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5FuU47N4wUL",
        "outputId": "814ec9f0-c6ae-49aa-ce18-bb21e29bdd1c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parquet reloaded:\n",
            "+-------+---+------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+-------------+-------------+-------------+\n",
            "|country| id|  name|signup_date| income|    plan|          signup_ts|year|month|is_india|income_rank_in_country|plan_priority|       region|population_bn|\n",
            "+-------+---+------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+-------------+-------------+-------------+\n",
            "|     PK|  8|Hassan| 2025-10-01|52000.0| premium|2025-10-01 00:00:00|2025|   10|       0|                     1|            3|         Asia|         0.24|\n",
            "|     UK|  4| Diana| 2025-09-30|39000.0|standard|2025-09-30 00:00:00|2025|    9|       0|                     1|            2|       Europe|         0.07|\n",
            "|     US|  2|   Bob| 2025-10-03|43000.0|standard|2025-10-03 00:00:00|2025|   10|       0|                     1|            2|North America|         0.33|\n",
            "|     AE|  6| Farid| 2025-10-02|31000.0|   basic|2025-10-02 00:00:00|2025|   10|       0|                     1|            1|         Asia|         0.01|\n",
            "|     IN|  5|  Esha| 2025-10-02|85000.0| premium|2025-10-02 00:00:00|2025|   10|       1|                     1|            3|         Asia|         1.42|\n",
            "|     IN|  3|Carlos| 2025-09-27|72000.0| premium|2025-09-27 00:00:00|2025|    9|       1|                     2|            3|         Asia|         1.42|\n",
            "|     IN|  1| Alice| 2025-10-01|56000.0| premium|2025-10-01 00:00:00|2025|   10|       1|                     3|            3|         Asia|         1.42|\n",
            "|     IN|  7|  Gita| 2025-09-29|46000.0|standard|2025-09-29 00:00:00|2025|    9|       1|                     4|            2|         Asia|         1.42|\n",
            "+-------+---+------+-----------+-------+--------+-------------------+----+-----+--------+----------------------+-------------+-------------+-------------+\n",
            "\n",
            "+------+-------+-------+-------------------+\n",
            "|  name|country| income|          signup_ts|\n",
            "+------+-------+-------+-------------------+\n",
            "|   Bob|     US|43000.0|2025-10-03 00:00:00|\n",
            "|  Esha|     IN|85000.0|2025-10-02 00:00:00|\n",
            "| Farid|     AE|31000.0|2025-10-02 00:00:00|\n",
            "| Alice|     IN|56000.0|2025-10-01 00:00:00|\n",
            "|Hassan|     PK|52000.0|2025-10-01 00:00:00|\n",
            "+------+-------+-------+-------------------+\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [signup_ts#37 DESC NULLS LAST], true, 0\n",
            "   +- Exchange rangepartitioning(signup_ts#37 DESC NULLS LAST, 4), ENSURE_REQUIREMENTS, [plan_id=3860]\n",
            "      +- Project [name#1, country#2, income#4, cast(signup_date#3 as timestamp) AS signup_ts#37]\n",
            "         +- Filter (isnotnull(signup_date#3) AND (cast(signup_date#3 as timestamp) >= 2025-10-01 00:00:00))\n",
            "            +- Scan ExistingRDD[id#0,name#1,country#2,signup_date#3,income#4,plan#5]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wA9GEv4s43bu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}